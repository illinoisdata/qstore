{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53db3541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from collections import defaultdict\n",
    "torch.set_printoptions(precision=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3a51e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need CUDA for quantization with bitsandbytes\n",
    "# Ensure enough GPU memory is available otherwise some tensors will not be quantized\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5fe6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \"\" # set huggingface token\n",
    "main_dir = \"/home/raunaks/\" # set directory where you want to save models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42938330",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "# model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "# model_name = \"qwen/qwen2-audio-7b-instruct\"\n",
    "# model_name = \"qwen/qwen2.5-7b-instruct\"\n",
    "# model_name = \"qwen/qwen2.5-vl-32b-instruct\"\n",
    "# model_name = \"deepseek-ai/deepseek-coder-33b-instruct\"\n",
    "# model_name = \"google/gemma-3-27b-it\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce10173a",
   "metadata": {},
   "source": [
    "For most models (e.g. Deepseek Coder 33B, Llama 3.1 8B, Mistral 7B), we can use `AutoModelForCausalLM.from_pretrained()`\n",
    "\n",
    "For some specific models (e.g. Qwen 2.5 VL, Qwen 2 Audio, Gemma 3) we may need to use their specific classes instead of `AutoModelForCausalLM` (imports listed below)\n",
    "\n",
    "Ensure total GPU memory is sufficient for quantization, otherwise fewer tensors will be quantized, and the downloaded model will be bigger (i.e. do not offload to CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8c244f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2AudioForConditionalGeneration, Gemma3ForConditionalGeneration, Qwen2_5_VLForConditionalGeneration\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                            torch_dtype=torch.bfloat16, # bfloat16 or float16\n",
    "                                            cache_dir=\"/projects/bdjx/rshah6/\", \n",
    "                                            device_map=\"auto\",\n",
    "                                            token=token\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd85238",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(main_dir + model_name + \"-bf16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f9829a",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                                torch_dtype=torch.bfloat16, # bfloat16 or float16\n",
    "                                                cache_dir=\"/projects/bdjx/rshah6/\", \n",
    "                                                device_map=\"auto\", \n",
    "                                                token=token,\n",
    "                                                load_in_8bit=True # will use LLM.int8() for quantization\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117caa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_model.save_pretrained(main_dir + model_name + \"-bf16-int8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
